{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3146b040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-genai\n",
      "  Using cached google_genai-1.26.0-py3-none-any.whl.metadata (42 kB)\n",
      "Collecting anyio<5.0.0,>=4.8.0 (from google-genai)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth<3.0.0,>=2.14.1 (from google-genai)\n",
      "  Using cached google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting httpx<1.0.0,>=0.28.1 (from google-genai)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydantic<3.0.0,>=2.0.0 (from google-genai)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in c:\\users\\hy608\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-genai) (2.32.3)\n",
      "Collecting tenacity<9.0.0,>=8.2.3 (from google-genai)\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting websockets<15.1.0,>=13.0.0 (from google-genai)\n",
      "  Downloading websockets-15.0.1-cp311-cp311-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in c:\\users\\hy608\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from google-genai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\hy608\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.10)\n",
      "Collecting sniffio>=1.1 (from anyio<5.0.0,>=4.8.0->google-genai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0.0,>=2.14.1->google-genai)\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.0,>=2.14.1->google-genai)\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.0,>=2.14.1->google-genai)\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\hy608\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2024.12.14)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0,>=0.28.1->google-genai)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.0.0->google-genai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.0.0->google-genai)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.0.0->google-genai)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hy608\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hy608\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.2.3)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Using cached google_genai-1.26.0-py3-none-any.whl (217 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 21.7 MB/s eta 0:00:00\n",
      "Using cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading websockets-15.0.1-cp311-cp311-win_amd64.whl (176 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: websockets, typing-inspection, tenacity, sniffio, pydantic-core, pyasn1, h11, cachetools, annotated-types, rsa, pydantic, pyasn1-modules, httpcore, anyio, httpx, google-auth, google-genai\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.9.0 cachetools-5.5.2 google-auth-2.40.3 google-genai-1.26.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 pydantic-2.11.7 pydantic-core-2.33.2 rsa-4.9.1 sniffio-1.3.1 tenacity-8.5.0 typing-inspection-0.4.1 websockets-15.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script websockets.exe is installed in 'C:\\Users\\hy608\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts pyrsa-decrypt.exe, pyrsa-encrypt.exe, pyrsa-keygen.exe, pyrsa-priv2pub.exe, pyrsa-sign.exe and pyrsa-verify.exe are installed in 'C:\\Users\\hy608\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script httpx.exe is installed in 'C:\\Users\\hy608\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\hy608\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "243d1611",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "this-day-in-history-07-19-1799-rosetta-stone-found.jpg is not a valid file path.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m genai\n\u001b[32m      3\u001b[39m client = genai.Client(api_key=\u001b[33m\"\u001b[39m\u001b[33mAIzaSyCY3GbJOrSKN_ttTN9HtCi-cgEXf_K6zEM\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m my_file = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthis-day-in-history-07-19-1799-rosetta-stone-found.jpg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m response = client.models.generate_content(\n\u001b[32m      8\u001b[39m     model=\u001b[33m\"\u001b[39m\u001b[33mgemini-2.5-flash\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m     contents=[my_file, \u001b[33m\"\u001b[39m\u001b[33mPlease only give the name of the object and the link to its wikipedia page(if it has one). Nothing else.\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     10\u001b[39m )\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hy608\\anaconda3\\envs\\automuseum\\Lib\\site-packages\\google\\genai\\files.py:628\u001b[39m, in \u001b[36mFiles.upload\u001b[39m\u001b[34m(self, file, config)\u001b[39m\n\u001b[32m    626\u001b[39m fs_path = os.fspath(file)\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fs_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(fs_path):\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a valid file path.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    629\u001b[39m file_obj.size_bytes = os.path.getsize(fs_path)\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file_obj.mime_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: this-day-in-history-07-19-1799-rosetta-stone-found.jpg is not a valid file path."
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key=\"AIzaSyCY3GbJOrSKN_ttTN9HtCi-cgEXf_K6zEM\")\n",
    "\n",
    "my_file = client.files.upload(file=\"this-day-in-history-07-19-1799-rosetta-stone-found.jpg\")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=[my_file, \"Please only give the name of the object and the link to its wikipedia page(if it has one). Nothing else.\"],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af8c2072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Rosetta Stone\n",
      "URL: https://en.wikipedia.org/wiki/Rosetta_Stone\n"
     ]
    }
   ],
   "source": [
    "def simple_parse(response_text):\n",
    "    \"\"\"Simple parsing for consistent format responses\"\"\"\n",
    "    lines = response_text.strip().split('\\n')\n",
    "    \n",
    "    title = lines[0].strip() if len(lines) > 0 else \"Unknown\"\n",
    "    url = lines[1].strip() if len(lines) > 1 else None\n",
    "    \n",
    "    # Validate URL\n",
    "    if url and not url.startswith('http'):\n",
    "        url = None\n",
    "    \n",
    "    return {\n",
    "        'title': title,\n",
    "        'url': url\n",
    "    }\n",
    "\n",
    "# Use the simple parser\n",
    "result = simple_parse(response.text)\n",
    "print(f\"Title: {result['title']}\")\n",
    "print(f\"URL: {result['url']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f05d2e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Rosetta Stone\n",
      "Introduction: The Rosetta Stone is a stele of granodiorite inscribed with three versions of a decree issued in 196 BC during the Ptolemaic dynasty of Egypt, on behalf of King Ptolemy V Epiphanes. The top and middle texts are in Ancient Egyptian using hieroglyphic and Demotic scripts, respectively, while the bottom is in Ancient Greek. The decree has only minor differences across the three versions, making the Rosetta Stone key to deciphering the Egyptian scripts....\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "\n",
    "class WikipediaScraper:\n",
    "    \"\"\"\n",
    "    Scraper for Wikipedia that retrieves information about objects identified in images.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.base_url = \"https://en.wikipedia.org/wiki/\"\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Auto-Museum/1.0 (Educational Project; contact@automuseum.example.com)'\n",
    "        }\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Remove references, citations and other wiki markup\"\"\"\n",
    "        text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "        text = re.sub(r'\\(listen\\)', '', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def get_article_content(self, title, url):\n",
    "        \"\"\"\n",
    "        Retrieve and parse Wikipedia article content for a given topic\n",
    "        \n",
    "        Args:\n",
    "            url (str): The Wikipedia article link\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing article sections and content\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Extract title\n",
    "            # title = soup.find(id=\"firstHeading\").text\n",
    "            \n",
    "            # Get the main content\n",
    "            content_div = soup.find(id=\"mw-content-text\")\n",
    "            \n",
    "            # Extract the content\n",
    "            paragraphs = []\n",
    "            for p in content_div.find_all('p'):\n",
    "                if p.text.strip():\n",
    "                    paragraphs.append(self.clean_text(p.text))\n",
    "            \n",
    "            return {\n",
    "                'title': title,\n",
    "                'url': url,\n",
    "                'paragraphs': paragraphs\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving Wikipedia content for url: {e}\")\n",
    "            return {\n",
    "                'title': None,\n",
    "                'url': url,\n",
    "                'paragraphs': [f\"Could not retrieve information about url.\"]\n",
    "            }\n",
    "\n",
    "# Example usage\n",
    "scraper = WikipediaScraper()\n",
    "fossil_info = scraper.get_article_content(\"Rosetta Stone\", \"https://en.wikipedia.org/wiki/Rosetta_Stone\")\n",
    "print(f\"Title: {fossil_info['title']}\")\n",
    "print(f\"Introduction: {fossil_info['paragraphs'][0]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0db824c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import json\n",
    "import ollama\n",
    "import os\n",
    "\n",
    "class RAGPipeline:\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
    "        # Load the sentence transformer model for creating embeddings\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "        # Initialize FAISS index\n",
    "        self.dimension = self.model.get_sentence_embedding_dimension()\n",
    "        self.index = faiss.IndexFlatL2(self.dimension)\n",
    "\n",
    "        # Storage for our text chunks and metadata\n",
    "        self.documents = []\n",
    "        self.metadata = {}\n",
    "\n",
    "    def add_wikipedia_content(self, wiki_data):\n",
    "        \"\"\"Process Wikipedia content and add to the vector database\"\"\"\n",
    "        # Process introduction\n",
    "        self.documents = wiki_data['paragraphs']\n",
    "        self.metadata = {'title': wiki_data['title'], 'url': wiki_data['url']}\n",
    "        self.object_name = wiki_data['title']\n",
    "\n",
    "        self._update_index()\n",
    "\n",
    "    def _update_index(self):\n",
    "        \"\"\"Update the FAISS index with the current documents\"\"\"\n",
    "        if not self.documents:\n",
    "            return\n",
    "\n",
    "        embeddings = self.model.encode(self.documents)\n",
    "        embeddings = np.array([embedding for embedding in embeddings]).astype('float32')\n",
    "\n",
    "        self.index = faiss.IndexFlatL2(self.dimension)\n",
    "        self.index.add(embeddings)\n",
    "\n",
    "    def retrieve(self, query, top_k=3):\n",
    "        \"\"\"Retrieve relevant context for a query\"\"\"\n",
    "        query_embedding = self.model.encode([query])[0].reshape(1, -1).astype('float32')\n",
    "        distances, indices = self.index.search(query_embedding, min(top_k, len(self.documents)))\n",
    "\n",
    "        results = []\n",
    "        for i, idx in enumerate(indices[0]):\n",
    "            if idx < len(self.documents):\n",
    "                results.append({\n",
    "                    'text': self.documents[idx],\n",
    "                    'metadata': self.metadata,\n",
    "                    'distance': float(distances[0][i])\n",
    "                })\n",
    "\n",
    "        return results\n",
    "\n",
    "    def generate_museum_description(self, model=\"llama3\"):\n",
    "        \"\"\"Generate a museum-style description using the LLM and retrieved context\"\"\"\n",
    "        context = self.retrieve(f\"Information about {self.object_name}\", top_k=5)\n",
    "        print(len(context))\n",
    "\n",
    "        context_text = \"\"\n",
    "        # context_text += f\"Source: {context[0]['metadata']['url']}\\n\"\n",
    "        for item in context:\n",
    "            context_text += item['text'] + \"\\n\\n\"\n",
    "\n",
    "        prompt = f'''\n",
    "        You are a museum curator writing an informative and engaging description plaque for an exhibit.\n",
    "\n",
    "        Object: {self.object_name}\n",
    "\n",
    "        Based on the following information, write a museum-style description plaque for this object.\n",
    "        The description should be informative, educational, and engaging for museum visitors.\n",
    "        Write in a professional tone similar to what would be found in a prestigious museum.\n",
    "\n",
    "        CONTEXTUAL INFORMATION:\n",
    "        {context_text}\n",
    "\n",
    "        REQUIREMENTS:\n",
    "        1. Begin with a catchy title (max 10 words)\n",
    "        2. The main description should be 150-200 words\n",
    "        3. Include key historical or scientific information\n",
    "        4. Make it accessible to general audience (grade 10 level)\n",
    "        5. Include 2-3 interesting facts that would surprise visitors\n",
    "        '''\n",
    "\n",
    "        try:\n",
    "            if len(context) == 1:\n",
    "                return {\n",
    "                    'title': f\"Exhibit: {self.object_name}\",\n",
    "                    'description': f\"Information about this {object_name} is currently being curated.\",\n",
    "                    'object_name': self.object_name,\n",
    "                    'sources': []\n",
    "                }\n",
    "\n",
    "\n",
    "            # Generate description using Ollama\n",
    "            response = ollama.generate(model=model, prompt=prompt)\n",
    "            description = response['response'].strip()\n",
    "\n",
    "            parts = description.split('\\n', 1)\n",
    "            title = parts[0].strip()\n",
    "            body = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "\n",
    "            return {\n",
    "                'title': title,\n",
    "                'description': body,\n",
    "                'object_name': self.object_name,\n",
    "                'sources': self.metadata['url']\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating museum description: {e}\")\n",
    "            return {\n",
    "                'title': f\"Exhibit: {self.object_name}\",\n",
    "                'description': f\"Information about this {self.object_name} is currently being curated.\",\n",
    "                'object_name': self.object_name,\n",
    "                'sources': []\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2060b07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "{\n",
      "  \"title\": \"**\\\"Unlocking the Secrets of the Ancient World: The Rosetta Stone\\\"**\",\n",
      "  \"description\": \"Discover one of history's most significant archaeological finds, the Rosetta Stone! This ancient stele holds the key to deciphering Egyptian hieroglyphics and is a testament to human ingenuity.\\n\\nIn 196 BC, during the Ptolemaic dynasty of Egypt, King Ptolemy V Epiphanes issued a decree inscribed on this granodiorite stone. The top register features ancient Egyptian hieroglyphs, while the middle text is written in Demotic script, and the bottom inscription is in Ancient Greek. This unique trilingual artifact allows us to understand the evolution of language and writing systems.\\n\\nInterestingly, the Rosetta Stone's inscriptions were only slightly modified across its three versions, making it a crucial tool for deciphering Egyptian scripts. The stone itself measures 112.3 cm high, 75.7 cm wide, and 28.4 cm thick, weighing approximately 760 kilograms.\\n\\nDid you know that the Rosetta Stone has inspired various language translation tools and software in modern times? It's a testament to the enduring power of ancient knowledge!\\n\\n**Fascinating Facts:**\\n\\n* The Rosetta Stone is not the only \\\"Rosetta stone\\\" in history. Other bilingual or trilingual epigraphical documents have been described as similar, allowing for the decipherment of ancient written scripts.\\n* A replica of the Rosetta Stone can be found in the King's Library of the British Museum, where visitors can touch and experience it as early 19th-century visitors did.\\n\\nCome and explore this incredible artifact, and uncover the secrets of the ancient world!\",\n",
      "  \"object_name\": \"Rosetta Stone\",\n",
      "  \"sources\": \"https://en.wikipedia.org/wiki/Rosetta_Stone\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scraper = WikipediaScraper()\n",
    "fossil_info = scraper.get_article_content(result['title'], result['url'])\n",
    "rag = RAGPipeline()\n",
    "rag.add_wikipedia_content(fossil_info)\n",
    "output = rag.generate_museum_description()\n",
    "print(json.dumps(output, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automuseum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
